{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33d3ae3-78f5-4514-a466-8f5165e084a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3882, 28, 28), Val: (524, 28, 28), Test: (624, 28, 28)\n",
      "Class 0: 388 Class 1: 3494\n",
      "Balanced counts: [3494 3494]\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 20s 2us/step\n",
      "Epoch 1/10\n",
      "874/874 [==============================] - 104s 111ms/step - loss: 0.0294 - accuracy: 0.8520 - precision: 0.9839 - recall: 0.7158 - val_loss: 0.0247 - val_accuracy: 0.8225 - val_precision: 0.9933 - val_recall: 0.7661 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "874/874 [==============================] - 100s 114ms/step - loss: 0.0207 - accuracy: 0.8706 - precision: 0.9905 - recall: 0.7484 - val_loss: 0.0193 - val_accuracy: 0.8569 - val_precision: 0.9906 - val_recall: 0.8149 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "874/874 [==============================] - 100s 115ms/step - loss: 0.0188 - accuracy: 0.8891 - precision: 0.9917 - recall: 0.7848 - val_loss: 0.0194 - val_accuracy: 0.8206 - val_precision: 1.0000 - val_recall: 0.7584 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "874/874 [==============================] - 95s 109ms/step - loss: 0.0171 - accuracy: 0.8987 - precision: 0.9922 - recall: 0.8037 - val_loss: 0.0238 - val_accuracy: 0.9046 - val_precision: 0.9775 - val_recall: 0.8920 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "874/874 [==============================] - 97s 111ms/step - loss: 0.0130 - accuracy: 0.9141 - precision: 0.9952 - recall: 0.8323 - val_loss: 0.0176 - val_accuracy: 0.8779 - val_precision: 0.9880 - val_recall: 0.8458 - lr: 2.0000e-04\n",
      "Epoch 6/10\n",
      "874/874 [==============================] - 98s 112ms/step - loss: 0.0113 - accuracy: 0.9300 - precision: 0.9974 - recall: 0.8623 - val_loss: 0.0176 - val_accuracy: 0.8740 - val_precision: 0.9879 - val_recall: 0.8406 - lr: 2.0000e-04\n",
      "Epoch 7/10\n",
      "874/874 [==============================] - 95s 109ms/step - loss: 0.0107 - accuracy: 0.9299 - precision: 0.9977 - recall: 0.8618 - val_loss: 0.0178 - val_accuracy: 0.8645 - val_precision: 0.9907 - val_recall: 0.8252 - lr: 2.0000e-04\n",
      "Epoch 8/10\n",
      "874/874 [==============================] - 97s 111ms/step - loss: 0.0100 - accuracy: 0.9347 - precision: 0.9977 - recall: 0.8715 - val_loss: 0.0184 - val_accuracy: 0.8893 - val_precision: 0.9882 - val_recall: 0.8612 - lr: 4.0000e-05\n",
      "Epoch 9/10\n",
      "874/874 [==============================] - 96s 110ms/step - loss: 0.0100 - accuracy: 0.9363 - precision: 0.9977 - recall: 0.8746 - val_loss: 0.0184 - val_accuracy: 0.8912 - val_precision: 0.9882 - val_recall: 0.8638 - lr: 4.0000e-05\n",
      "Epoch 10/10\n",
      "874/874 [==============================] - 95s 109ms/step - loss: 0.0098 - accuracy: 0.9393 - precision: 0.9984 - recall: 0.8801 - val_loss: 0.0185 - val_accuracy: 0.8912 - val_precision: 0.9882 - val_recall: 0.8638 - lr: 8.0000e-06\n",
      "66/66 [==============================] - 7s 99ms/step\n",
      "ðŸ” Validation Predictions:\n",
      "Predicted: [166 358]\n",
      "Actual: [135 389]\n",
      "Epoch 1/10\n",
      "874/874 [==============================] - 146s 163ms/step - loss: 0.9599 - accuracy: 0.6344 - precision_1: 0.5821 - recall_1: 0.9525 - val_loss: 0.2822 - val_accuracy: 0.8492 - val_precision_1: 0.8384 - val_recall_1: 0.9871 - lr: 1.0000e-06\n",
      "Epoch 2/10\n",
      "874/874 [==============================] - 143s 163ms/step - loss: 0.1867 - accuracy: 0.8278 - precision_1: 0.8257 - recall_1: 0.8311 - val_loss: 0.2073 - val_accuracy: 0.8779 - val_precision_1: 0.8753 - val_recall_1: 0.9743 - lr: 1.0000e-06\n",
      "Epoch 3/10\n",
      "874/874 [==============================] - 141s 162ms/step - loss: 0.1029 - accuracy: 0.8310 - precision_1: 0.9123 - recall_1: 0.7324 - val_loss: 0.0611 - val_accuracy: 0.8969 - val_precision_1: 0.9539 - val_recall_1: 0.9049 - lr: 1.0000e-06\n",
      "Epoch 4/10\n",
      "874/874 [==============================] - 143s 163ms/step - loss: 0.0827 - accuracy: 0.8327 - precision_1: 0.9291 - recall_1: 0.7204 - val_loss: 0.0422 - val_accuracy: 0.8702 - val_precision_1: 0.9707 - val_recall_1: 0.8509 - lr: 1.0000e-06\n",
      "Epoch 5/10\n",
      "874/874 [==============================] - 143s 163ms/step - loss: 0.0625 - accuracy: 0.8442 - precision_1: 0.9449 - recall_1: 0.7310 - val_loss: 0.0407 - val_accuracy: 0.8531 - val_precision_1: 0.9845 - val_recall_1: 0.8149 - lr: 1.0000e-06\n",
      "Epoch 6/10\n",
      "874/874 [==============================] - 143s 163ms/step - loss: 0.0558 - accuracy: 0.8470 - precision_1: 0.9502 - recall_1: 0.7324 - val_loss: 0.0372 - val_accuracy: 0.8378 - val_precision_1: 0.9810 - val_recall_1: 0.7969 - lr: 1.0000e-06\n",
      "Epoch 7/10\n",
      "874/874 [==============================] - 143s 163ms/step - loss: 0.0502 - accuracy: 0.8393 - precision_1: 0.9506 - recall_1: 0.7158 - val_loss: 0.0336 - val_accuracy: 0.8340 - val_precision_1: 0.9840 - val_recall_1: 0.7892 - lr: 1.0000e-06\n",
      "Epoch 8/10\n",
      "874/874 [==============================] - 142s 163ms/step - loss: 0.0416 - accuracy: 0.8482 - precision_1: 0.9634 - recall_1: 0.7238 - val_loss: 0.0312 - val_accuracy: 0.8282 - val_precision_1: 0.9870 - val_recall_1: 0.7789 - lr: 1.0000e-06\n",
      "Epoch 9/10\n",
      "874/874 [==============================] - 142s 163ms/step - loss: 0.0330 - accuracy: 0.8598 - precision_1: 0.9715 - recall_1: 0.7413 - val_loss: 0.0284 - val_accuracy: 0.8302 - val_precision_1: 0.9902 - val_recall_1: 0.7789 - lr: 1.0000e-06\n",
      "Epoch 10/10\n",
      "874/874 [==============================] - 142s 163ms/step - loss: 0.0289 - accuracy: 0.8651 - precision_1: 0.9797 - recall_1: 0.7456 - val_loss: 0.0249 - val_accuracy: 0.8454 - val_precision_1: 0.9904 - val_recall_1: 0.7995 - lr: 1.0000e-06\n",
      "\n",
      "ðŸ“Š Train Set Evaluation:\n",
      "874/874 [==============================] - 90s 103ms/step - loss: 0.0128 - accuracy: 0.9197 - precision_1: 1.0000 - recall_1: 0.8394\n",
      "\n",
      "ðŸ“Š Test Set Evaluation:\n",
      "78/78 [==============================] - 8s 102ms/step - loss: 0.0853 - accuracy: 0.8221 - precision_1: 0.8720 - recall_1: 0.8385\n",
      "78/78 [==============================] - 9s 102ms/step\n",
      "\n",
      "ðŸ“ˆ Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75       234\n",
      "           1       0.83      0.89      0.86       390\n",
      "\n",
      "    accuracy                           0.82       624\n",
      "   macro avg       0.82      0.80      0.81       624\n",
      "weighted avg       0.82      0.82      0.82       624\n",
      "\n",
      "ðŸ“‰ Confusion Matrix:\n",
      "[[165  69]\n",
      " [ 41 349]]\n",
      "ðŸ§  ROC AUC Score: 0.9070238877931186\n"
     ]
    }
   ],
   "source": [
    "# âœ… Step 0: Imports\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"  # (Optional) Limit CPU thread usage\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# âœ… Step 1: Load data and flatten labels\n",
    "data = np.load(\"C:/Users/ishsi/Downloads/pneumoniamnist.npz\")\n",
    "x_train, y_train = data[\"train_images\"], data[\"train_labels\"]\n",
    "x_val, y_val = data[\"val_images\"], data[\"val_labels\"]\n",
    "x_test, y_test = data[\"test_images\"], data[\"test_labels\"]\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_val = y_val.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "print(f\"Train: {x_train.shape}, Val: {x_val.shape}, Test: {x_test.shape}\")\n",
    "print(\"Class 0:\", np.sum(y_train == 0), \"Class 1:\", np.sum(y_train == 1))\n",
    "\n",
    "# âœ… Step 2: Oversample class 0\n",
    "x_train_0 = x_train[y_train == 0]\n",
    "x_train_1 = x_train[y_train == 1]\n",
    "y_train_0 = y_train[y_train == 0]\n",
    "y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "x_train_0_up, y_train_0_up = resample(\n",
    "    x_train_0, y_train_0,\n",
    "    replace=True,\n",
    "    n_samples=len(y_train_1),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "x_train_bal = np.concatenate([x_train_0_up, x_train_1])\n",
    "y_train_bal = np.concatenate([y_train_0_up, y_train_1])\n",
    "\n",
    "shuffle_idx = np.random.permutation(len(y_train_bal))\n",
    "x_train_bal = x_train_bal[shuffle_idx]\n",
    "y_train_bal = y_train_bal[shuffle_idx]\n",
    "\n",
    "print(\"Balanced counts:\", np.bincount(y_train_bal.astype(int)))\n",
    "\n",
    "# âœ… Step 3: Preprocessing function\n",
    "def preprocess(x, y):\n",
    "    x = tf.image.resize(x[..., tf.newaxis], (224, 224))\n",
    "    x = tf.image.grayscale_to_rgb(x)\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    return x, y\n",
    "\n",
    "# âœ… Step 4: Create datasets with batch size 8\n",
    "batch_size = 8\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train_bal, y_train_bal))\n",
    "train_ds = train_ds.shuffle(1000).map(preprocess).map(lambda x, y: (tf.image.random_flip_left_right(x), y)).batch(batch_size)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).map(preprocess).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).map(preprocess).batch(batch_size)\n",
    "\n",
    "# âœ… Step 5: Custom Focal Loss with logits\n",
    "def binary_focal_loss(gamma=2.0, alpha=0.25, from_logits=False):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        if from_logits:\n",
    "            y_pred = tf.nn.sigmoid(y_pred)\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, eps, 1. - eps)\n",
    "        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        alpha_factor = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n",
    "        modulating_factor = tf.pow((1 - p_t), gamma)\n",
    "        return tf.reduce_mean(-alpha_factor * modulating_factor * tf.math.log(p_t))\n",
    "    return loss_fn\n",
    "\n",
    "# âœ… Step 6: Build model using MobileNetV2\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze for initial training\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1)  # No sigmoid (logits output)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=binary_focal_loss(gamma=2.0, alpha=0.25, from_logits=True),\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# âœ… Step 7: Train\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# âœ… Step 8: Debug prediction distribution\n",
    "val_probs = tf.nn.sigmoid(model.predict(val_ds)).numpy()\n",
    "val_preds = (val_probs > 0.5).astype(int)\n",
    "val_true = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "\n",
    "print(\"ðŸ” Validation Predictions:\")\n",
    "print(\"Predicted:\", np.bincount(val_preds.flatten()))\n",
    "print(\"Actual:\", np.bincount(val_true.astype(int)))\n",
    "\n",
    "# âœ… Step 9: Fine-tune top layers\n",
    "base_model.trainable = True\n",
    "fine_tune_at = 100\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-6),\n",
    "    loss=binary_focal_loss(gamma=2.0, alpha=0.25, from_logits=True),\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# âœ… Step 10: Evaluation\n",
    "print(\"\\nðŸ“Š Train Set Evaluation:\")\n",
    "model.evaluate(train_ds)\n",
    "\n",
    "print(\"\\nðŸ“Š Test Set Evaluation:\")\n",
    "model.evaluate(test_ds)\n",
    "\n",
    "# âœ… Step 11: Test metrics\n",
    "test_probs = tf.nn.sigmoid(model.predict(test_ds)).numpy()\n",
    "test_preds = (test_probs > 0.5).astype(int)\n",
    "test_true = np.concatenate([y for _, y in test_ds], axis=0)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Classification Report (Test):\")\n",
    "print(classification_report(test_true, test_preds))\n",
    "print(\"ðŸ“‰ Confusion Matrix:\")\n",
    "print(confusion_matrix(test_true, test_preds))\n",
    "print(\"ðŸ§  ROC AUC Score:\", roc_auc_score(test_true, test_probs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pneumonia-env)",
   "language": "python",
   "name": "pneumonia-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
